{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Work on this before the next lecture on 8 May. We will talk about questions, comments, and solutions during the exercise after the third lecture.\n",
    "\n",
    "Please do form study groups! When you do, make sure you can explain everything in your own words, do not simply copy&paste from others.\n",
    "\n",
    "The solutions to a lot of these problems can probably be found with Google. Please don't. You will not learn a lot by copy&pasting from the internet.\n",
    "\n",
    "If you want to get credit/examination on this course please upload your work to **your GitHub repository** for this course **before** the next lecture starts and post a link to your repository [in this thread](). If you worked on things together with others please add their names to the notebook so we can see who formed groups.\n",
    "\n",
    "---\n",
    "\n",
    "These are some useful default imports for plotting and [`numpy`](http://www.numpy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "You can use PCA as a form of noise filtering. The idea why this might work goes like this: overall the PCA components representing the noise will explain less of the variance in the dataset than the components related to the actual data. So if you keep only the high variance components you can remove (some) of the noise.\n",
    "\n",
    "* plot some of the digits before and after the noise has been added\n",
    "* fit a `PCA` transformer to the digit dataset\n",
    "* experiment with the number of components you need to get a good reconstruction of the digits that supresses the noise.\n",
    "* (bonus) is there a quantitative way to pick the best number of components?\n",
    "\n",
    "(Note: You can use this for high dimensional tabular data as well. Instead of training your classifier on all the features you can first transform your data and only use the first \"few\" components, thereby removing the noise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# fluctuate each pixel's intensity by drawing from a gaussian\n",
    "# with mean equal to the pixels value and std=4\n",
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Apply PCA to a dataset that contains two gaussian blobs. Imagine this is a toy version of a real problem where you are trying to separate the dark from the light points. Before using the `PCA` transformer make a prediction for the two principle components from your intuition.\n",
    "\n",
    "* draw the principle components PCA finds\n",
    "* reduce the dimensionality of the data to one dimension using the `transform` method of the `PCA` transformer\n",
    "* plot your transformed dataset, can you separate the two classes using only the first principle component?\n",
    "* A technique related to PCA that makes use of the class labels is called linear discriminant analysis (LDA). How does LDA perform on this problem?\n",
    "* Can you explain why PCA fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "m1 = (1,1)\n",
    "m2 = (1,4)\n",
    "C = ((1,0.75), (0.75,1))\n",
    "g1 = np.random.multivariate_normal((0, -2), C, N)\n",
    "g2 = np.random.multivariate_normal((0, +2), C, N)\n",
    "\n",
    "X = np.vstack((g1, g2))\n",
    "y = np.hstack((np.ones(N), np.zeros(N)))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Create a simple Auto Encoder using `keras`. By forcing the input through a bottleneck we can hope that the auto encoder focusses on interesting structure in the data and ignores noise. Maybe auto encoders can be used to perform denoising.\n",
    "\n",
    "* create your own auto encoder setup or start from the one shown in the lecture. You might have to construct a auto encoder that has more layers or uses more powerful transformations like `Conv2D`, `MaxPooling2D` and `Upsampling2D`.\n",
    "* make sure to load the MNIST dataset from `keras` and not the digits dataset from scikit-learn. Create a noisy version of your digits by drawing pixel values from a gaussian with mean equal to each pixel's intensity and a small standard deviation (tip: start with little/no noise to debug your auto encoder and then slowly increase the noise)\n",
    "* how do you need to modify the learning setup compared to the lecture where we use `X_train` both as input and as output?\n",
    "\n",
    "(Bonus: can you learn an \"auto encoder\" that rotates clean digits by 90degrees? This shuold start you off thinking about using neural networks as general purpose \"transformers\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (Bonus) Question 4\n",
    "\n",
    "Using your auto encoder architecture from the previous question, a clean MNIST dataset, and the ideas from the \"visual search\" notebook can you create a search engine that can find images similar to the query image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
